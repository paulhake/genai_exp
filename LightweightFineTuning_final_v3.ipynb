{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import libs\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, \\\n",
    "        Trainer, TrainingArguments, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load gpt2 model and configure tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model_gpt = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"openai-community/gpt2\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},  # For converting predictions to strings\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "model_gpt.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for param in model_gpt.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# examine model structure\n",
    "print(model_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c7409ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (2.18.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (2.1.4)\n",
      "Collecting requests>=2.32.1 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (3.9.3)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from requests>=2.32.1->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from requests>=2.32.1->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from requests>=2.32.1->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from requests>=2.32.1->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: requests, huggingface-hub, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 0.20.2\n",
      "    Uninstalling huggingface_hub-0.20.2:\n",
      "      Successfully uninstalled huggingface_hub-0.20.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.18.0\n",
      "    Uninstalling datasets-2.18.0:\n",
      "      Successfully uninstalled datasets-2.18.0\n",
      "Successfully installed datasets-2.19.2 huggingface-hub-0.23.3 requests-2.32.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since imdb couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /Users/paulhake/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31 (last modified on Tue Mar 19 16:07:35 2024).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#load imdb dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize all the examples\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"text\"],padding=\"max_length\", truncation=True)\n",
    "tokenized_dataset = dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Inspect the available columns in the dataset\n",
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0942ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test set and sub sample 500 for faster training and inferencing\n",
    "train = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(500))\n",
    "test = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67fb1cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.19.5 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.1.post1 threadpoolctl-3.4.0\n"
     ]
    }
   ],
   "source": [
    "#install scikit for accuracy evaluation\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa8f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ad8c609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 125/125 [25:39<00:00,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5870361924171448, 'eval_accuracy': 0.756, 'eval_runtime': 635.1798, 'eval_samples_per_second': 0.787, 'eval_steps_per_second': 0.197, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [25:40<00:00, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1540.7141, 'train_samples_per_second': 0.325, 'train_steps_per_second': 0.081, 'train_loss': 0.7883529052734375, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#train model head\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_trainer\",\n",
    "    per_device_eval_batch_size=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "gpt_trainer = Trainer(\n",
    "    model=model_gpt,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    #data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "gpt_eval = gpt_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eed1539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.7883529052734375, metrics={'train_runtime': 1540.7141, 'train_samples_per_second': 0.325, 'train_steps_per_second': 0.081, 'train_loss': 0.7883529052734375, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d2d9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_trainer.save_model(\"base_gpt_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f8d3e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [10:28<00:00,  5.03s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When I unsuspectedly rented A Thousand Acres...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is the latest entry in the long series of...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so frustrating. Everything seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was truly and wonderfully surprised at \"O' B...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label\n",
       "0    When I unsuspectedly rented A Thousand Acres...      1                1\n",
       "1  This is the latest entry in the long series of...      1                1\n",
       "2  This movie was so frustrating. Everything seem...      0                0\n",
       "3  I was truly and wonderfully surprised at \"O' B...      1                1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(test)\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Replace <br /> tags in the text with spaces\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"<br />\", \" \")\n",
    "\n",
    "# Add the model predictions to the dataframe\n",
    "predictions = gpt_trainer.predict(test)\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3900663c",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7159d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_trained = AutoModelForSequenceClassification.from_pretrained(\"base_gpt_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bb34ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32,\n",
    "    target_modules=['c_attn', 'c_proj'],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    fan_in_fan_out=True,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model_base_trained, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6da10d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                       \n",
      "\u001b[A                                              \n",
      "\n",
      "  0%|          | 0/125 [48:21<?, ?it/s]          \n",
      "\u001b[A\n",
      "\u001b[ACheckpoint destination directory ./loraModel/checkpoint-125 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /base_gpt_model/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15ec834a0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 359d573c-27ca-4405-b111-561f484e2fb6)') - silently ignoring the lookup for the file config.json in base_gpt_model.\n",
      "  warnings.warn(\n",
      "/Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in base_gpt_model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "                                       \n",
      "100%|██████████| 125/125 [48:00<00:00, 23.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35950732231140137, 'eval_accuracy': 0.886, 'eval_runtime': 692.1093, 'eval_samples_per_second': 0.722, 'eval_steps_per_second': 0.181, 'epoch': 1.0}\n",
      "{'train_runtime': 2880.1857, 'train_samples_per_second': 0.174, 'train_steps_per_second': 0.043, 'train_loss': 0.5166582641601563, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.5166582641601563, metrics={'train_runtime': 2880.1857, 'train_samples_per_second': 0.174, 'train_steps_per_second': 0.043, 'train_loss': 0.5166582641601563, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_lora = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./loraModel\",\n",
    "        learning_rate=1e-3,\n",
    "        # Reduce the batch size if you don't have enough memory\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_lora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "733b4d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [11:48<00:00,  5.67s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.35950732231140137,\n",
       " 'eval_accuracy': 0.886,\n",
       " 'eval_runtime': 714.3845,\n",
       " 'eval_samples_per_second': 0.7,\n",
       " 'eval_steps_per_second': 0.175,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_lora.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c5afae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 812,544 || all params: 125,253,888 || trainable%: 0.6487175871139426\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c934fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /base_gpt_model/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15ec88cb0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 48f0d55f-565e-4a13-adb4-e3c653e12e01)') - silently ignoring the lookup for the file config.json in base_gpt_model.\n",
      "  warnings.warn(\n",
      "/Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in base_gpt_model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_model.save_pretrained(\"gpt2_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f059b797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2_lora_tokenizer/tokenizer_config.json',\n",
       " 'gpt2_lora_tokenizer/special_tokens_map.json',\n",
       " 'gpt2_lora_tokenizer/vocab.json',\n",
       " 'gpt2_lora_tokenizer/merges.txt',\n",
       " 'gpt2_lora_tokenizer/added_tokens.json',\n",
       " 'gpt2_lora_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"gpt2_lora_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulhake/opt/anaconda3/envs/ugen/lib/python3.12/site-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /gpt2_lora/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15ed97a10>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: e35b5fa0-7120-444d-bbbf-36d8a6089135)') - silently ignoring the lookup for the file tokenizer_config.json in gpt2_lora.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"gpt2_lora\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2_lora_tokenizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [11:49<00:00,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35950732231140137, 'eval_accuracy': 0.886, 'eval_runtime': 714.9352, 'eval_samples_per_second': 0.699, 'eval_steps_per_second': 0.175}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#saved lora model evaluation\n",
    "trainer_lora = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./loraModel\",\n",
    "        learning_rate=1e-3,\n",
    "        # Reduce the batch size if you don't have enough memory\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "eval_results = trainer_lora.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45126825",
   "metadata": {},
   "source": [
    "## comparison of original model and peft model\n",
    "Base model accuracy = 0.756\n",
    "Lora model accuracy = 0.886\n",
    "\n",
    "\n",
    "\n",
    "### original base results from original gpt base model above:\n",
    "{'eval_loss': 0.5870361924171448, \n",
    " 'eval_accuracy': 0.756, \n",
    " 'eval_runtime': 635.1798, \n",
    " 'eval_samples_per_second': 0.787, \n",
    " 'eval_steps_per_second': 0.197, \n",
    " 'epoch': 1.0}\n",
    "100%|██████████| 125/125 [25:40<00:00, 12.33s/it]\n",
    "\n",
    "\n",
    "### eval results from lora model:\n",
    "{'eval_loss': 0.35950732231140137,\n",
    " 'eval_accuracy': 0.886,\n",
    " 'eval_runtime': 714.3845,\n",
    " 'eval_samples_per_second': 0.7,\n",
    " 'eval_steps_per_second': 0.175,\n",
    " 'epoch': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7afb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
